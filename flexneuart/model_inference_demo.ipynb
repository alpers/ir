{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the repo is cloned, all necessary packages are installed, including calling the script:\n",
    "\n",
    "```./install_packages.sh```\n",
    "\n",
    "and the code is compiled:\n",
    "\n",
    "```./build.sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing directory to the repo root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alp/Documents/FlexNeuART\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading demo data\n",
    "\n",
    "1. Download [this file from our Google Drive](https://drive.google.com/file/d/1mDa6J4hNYPyqlS8hVi6bykSbAOMKsDwe/view?usp=sharing) and copy it to the source root directory, where it should be unpacked. As a result, a source directory should contain a sub-directory ``collections/msmarco_doc``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: statistics on downloaded data should look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "Checking data sub-directory: bitext\n",
      "Checking data sub-directory: dev\n",
      "Checking data sub-directory: dev_official\n",
      "Checking data sub-directory: docs\n",
      "Found indexable data file: docs/AnswerFields.jsonl.gz\n",
      "Checking data sub-directory: test2019\n",
      "Checking data sub-directory: test2020\n",
      "Checking data sub-directory: train_fusion\n",
      "Found query file: bitext/QuestionFields.jsonl\n",
      "Found query file: dev/QuestionFields.jsonl\n",
      "Found query file: dev_official/QuestionFields.jsonl\n",
      "Found query file: test2019/QuestionFields.jsonl\n",
      "Found query file: test2020/QuestionFields.jsonl\n",
      "Found query file: train_fusion/QuestionFields.jsonl\n",
      "getIndexQueryDataInfo return value:  docs AnswerFields.jsonl.gz ,bitext,dev,dev_official,test2019,test2020,train_fusion QuestionFields.jsonl\n",
      "Using the data input files: AnswerFields.jsonl.gz, QuestionFields.jsonl\n",
      "Index dirs: docs\n",
      "Query dirs:  bitext dev dev_official test2019 test2020 train_fusion\n",
      "Queries/questions:\n",
      "bitext 352013\n",
      "dev 5000\n",
      "dev_official 5193\n",
      "test2019 43\n",
      "test2020 45\n",
      "train_fusion 10000\n",
      "Documents/passages/answers:\n",
      "docs 3213802\n"
     ]
    }
   ],
   "source": [
    "!scripts/report/get_basic_collect_stat.sh msmarco_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (each step takes a few hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lucene index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection root: collections\n",
      "==========================================================================\n",
      "Data directory: collections/msmarco_doc/input_data\n",
      "Index directory: collections/msmarco_doc/lucene_index\n",
      "==========================================================================\n",
      "Checking data sub-directory: bitext\n",
      "Checking data sub-directory: dev\n",
      "Checking data sub-directory: dev_official\n",
      "Checking data sub-directory: docs\n",
      "Found indexable data file: docs/AnswerFields.jsonl.gz\n",
      "Checking data sub-directory: test2019\n",
      "Checking data sub-directory: test2020\n",
      "Checking data sub-directory: train_fusion\n",
      "Found query file: bitext/QuestionFields.jsonl\n",
      "Found query file: dev/QuestionFields.jsonl\n",
      "Found query file: dev_official/QuestionFields.jsonl\n",
      "Found query file: test2019/QuestionFields.jsonl\n",
      "Found query file: test2020/QuestionFields.jsonl\n",
      "Found query file: train_fusion/QuestionFields.jsonl\n",
      "Using the data input file: AnswerFields.jsonl.gz\n",
      "JAVA_OPTS=-Xms8388608k -Xmx14680064k -server\n",
      "Creating a new Lucene index, maximum # of docs to process: 2147483647\n",
      "Input file name: collections/msmarco_doc/input_data/docs/AnswerFields.jsonl.gz\n",
      "Indexed 10000 docs\n",
      "Indexed 20000 docs\n",
      "Indexed 30000 docs\n",
      "Indexed 40000 docs\n",
      "Indexed 50000 docs\n",
      "Committing\n",
      "Indexed 60000 docs\n",
      "Indexed 70000 docs\n",
      "Indexed 80000 docs\n",
      "Indexed 90000 docs\n",
      "Indexed 100000 docs\n",
      "Committing\n",
      "Indexed 110000 docs\n",
      "Indexed 120000 docs\n",
      "Indexed 130000 docs\n",
      "Indexed 140000 docs\n",
      "Indexed 150000 docs\n",
      "Committing\n",
      "Indexed 160000 docs\n",
      "Indexed 170000 docs\n",
      "Indexed 180000 docs\n",
      "Indexed 190000 docs\n",
      "Indexed 200000 docs\n",
      "Committing\n",
      "Indexed 210000 docs\n",
      "Indexed 220000 docs\n",
      "Indexed 230000 docs\n",
      "Indexed 240000 docs\n",
      "Indexed 250000 docs\n",
      "Committing\n",
      "Indexed 260000 docs\n",
      "Indexed 270000 docs\n",
      "Indexed 280000 docs\n",
      "Indexed 290000 docs\n",
      "Indexed 300000 docs\n",
      "Committing\n",
      "Indexed 310000 docs\n",
      "Indexed 320000 docs\n",
      "Indexed 330000 docs\n",
      "Indexed 340000 docs\n",
      "Indexed 350000 docs\n",
      "Committing\n",
      "Indexed 360000 docs\n",
      "Indexed 370000 docs\n",
      "Indexed 380000 docs\n",
      "Indexed 390000 docs\n",
      "Indexed 400000 docs\n",
      "Committing\n",
      "Indexed 410000 docs\n",
      "Indexed 420000 docs\n",
      "Indexed 430000 docs\n",
      "Indexed 440000 docs\n",
      "Indexed 450000 docs\n",
      "Committing\n",
      "Indexed 460000 docs\n",
      "Indexed 470000 docs\n",
      "Indexed 480000 docs\n",
      "Indexed 490000 docs\n",
      "Indexed 500000 docs\n",
      "Committing\n",
      "Indexed 510000 docs\n",
      "Indexed 520000 docs\n",
      "Indexed 530000 docs\n",
      "Indexed 540000 docs\n",
      "Indexed 550000 docs\n",
      "Committing\n",
      "Indexed 560000 docs\n",
      "Indexed 570000 docs\n",
      "Indexed 580000 docs\n",
      "Indexed 590000 docs\n",
      "Indexed 600000 docs\n",
      "Committing\n",
      "Indexed 610000 docs\n",
      "Indexed 620000 docs\n",
      "Indexed 630000 docs\n",
      "Indexed 640000 docs\n",
      "Indexed 650000 docs\n",
      "Committing\n",
      "Indexed 660000 docs\n",
      "Indexed 670000 docs\n",
      "Indexed 680000 docs\n",
      "Indexed 690000 docs\n",
      "Indexed 700000 docs\n",
      "Committing\n",
      "Indexed 710000 docs\n",
      "Indexed 720000 docs\n",
      "Indexed 730000 docs\n",
      "Indexed 740000 docs\n",
      "Indexed 750000 docs\n",
      "Committing\n",
      "Indexed 760000 docs\n",
      "Indexed 770000 docs\n",
      "Indexed 780000 docs\n",
      "Indexed 790000 docs\n",
      "Indexed 800000 docs\n",
      "Committing\n",
      "Indexed 810000 docs\n",
      "Indexed 820000 docs\n",
      "Indexed 830000 docs\n",
      "Indexed 840000 docs\n",
      "Indexed 850000 docs\n",
      "Committing\n",
      "Indexed 860000 docs\n",
      "Indexed 870000 docs\n",
      "Indexed 880000 docs\n",
      "Indexed 890000 docs\n",
      "Indexed 900000 docs\n",
      "Committing\n",
      "Indexed 910000 docs\n",
      "Indexed 920000 docs\n",
      "Indexed 930000 docs\n",
      "Indexed 940000 docs\n",
      "Indexed 950000 docs\n",
      "Committing\n",
      "Indexed 960000 docs\n",
      "Indexed 970000 docs\n",
      "Indexed 980000 docs\n",
      "Indexed 990000 docs\n",
      "Indexed 1000000 docs\n",
      "Committing\n",
      "Indexed 1010000 docs\n",
      "Indexed 1020000 docs\n",
      "Indexed 1030000 docs\n",
      "Indexed 1040000 docs\n",
      "Indexed 1050000 docs\n",
      "Committing\n",
      "Indexed 1060000 docs\n",
      "Indexed 1070000 docs\n",
      "Indexed 1080000 docs\n",
      "Indexed 1090000 docs\n",
      "Indexed 1100000 docs\n",
      "Committing\n",
      "Indexed 1110000 docs\n",
      "Indexed 1120000 docs\n",
      "Indexed 1130000 docs\n",
      "Indexed 1140000 docs\n",
      "Indexed 1150000 docs\n",
      "Committing\n",
      "Indexed 1160000 docs\n",
      "Indexed 1170000 docs\n",
      "Indexed 1180000 docs\n",
      "Indexed 1190000 docs\n",
      "Indexed 1200000 docs\n",
      "Committing\n",
      "Indexed 1210000 docs\n",
      "Indexed 1220000 docs\n",
      "Indexed 1230000 docs\n",
      "Indexed 1240000 docs\n",
      "Indexed 1250000 docs\n",
      "Committing\n",
      "Indexed 1260000 docs\n",
      "Indexed 1270000 docs\n",
      "Indexed 1280000 docs\n",
      "Indexed 1290000 docs\n",
      "Indexed 1300000 docs\n",
      "Committing\n",
      "Indexed 1310000 docs\n",
      "Indexed 1320000 docs\n",
      "Indexed 1330000 docs\n",
      "Indexed 1340000 docs\n",
      "Indexed 1350000 docs\n",
      "Committing\n",
      "Indexed 1360000 docs\n",
      "Indexed 1370000 docs\n",
      "Indexed 1380000 docs\n",
      "Indexed 1390000 docs\n",
      "Indexed 1400000 docs\n",
      "Committing\n",
      "Indexed 1410000 docs\n",
      "Indexed 1420000 docs\n",
      "Indexed 1430000 docs\n",
      "Indexed 1440000 docs\n",
      "Indexed 1450000 docs\n",
      "Committing\n",
      "Indexed 1460000 docs\n",
      "Indexed 1470000 docs\n",
      "Indexed 1480000 docs\n",
      "Indexed 1490000 docs\n",
      "Indexed 1500000 docs\n",
      "Committing\n",
      "Indexed 1510000 docs\n",
      "Indexed 1520000 docs\n",
      "Indexed 1530000 docs\n",
      "Indexed 1540000 docs\n",
      "Indexed 1550000 docs\n",
      "Committing\n",
      "Indexed 1560000 docs\n",
      "Indexed 1570000 docs\n",
      "Indexed 1580000 docs\n",
      "Indexed 1590000 docs\n",
      "Indexed 1600000 docs\n",
      "Committing\n",
      "Indexed 1610000 docs\n",
      "Indexed 1620000 docs\n",
      "Indexed 1630000 docs\n",
      "Indexed 1640000 docs\n",
      "Indexed 1650000 docs\n",
      "Committing\n",
      "Indexed 1660000 docs\n",
      "Indexed 1670000 docs\n",
      "Indexed 1680000 docs\n",
      "Indexed 1690000 docs\n",
      "Indexed 1700000 docs\n",
      "Committing\n",
      "Indexed 1710000 docs\n",
      "Indexed 1720000 docs\n",
      "Indexed 1730000 docs\n",
      "Indexed 1740000 docs\n",
      "Indexed 1750000 docs\n",
      "Committing\n",
      "Indexed 1760000 docs\n",
      "Indexed 1770000 docs\n",
      "Indexed 1780000 docs\n",
      "Indexed 1790000 docs\n",
      "Indexed 1800000 docs\n",
      "Committing\n",
      "Indexed 1810000 docs\n",
      "Indexed 1820000 docs\n",
      "Indexed 1830000 docs\n",
      "Indexed 1840000 docs\n",
      "Indexed 1850000 docs\n",
      "Committing\n",
      "Indexed 1860000 docs\n",
      "Indexed 1870000 docs\n",
      "Indexed 1880000 docs\n",
      "Indexed 1890000 docs\n",
      "Indexed 1900000 docs\n",
      "Committing\n",
      "Indexed 1910000 docs\n",
      "Indexed 1920000 docs\n",
      "Indexed 1930000 docs\n",
      "Indexed 1940000 docs\n",
      "Indexed 1950000 docs\n",
      "Committing\n",
      "Indexed 1960000 docs\n",
      "Indexed 1970000 docs\n",
      "Indexed 1980000 docs\n",
      "Indexed 1990000 docs\n",
      "Indexed 2000000 docs\n",
      "Committing\n",
      "Indexed 2010000 docs\n",
      "Indexed 2020000 docs\n",
      "Indexed 2030000 docs\n",
      "Indexed 2040000 docs\n",
      "Indexed 2050000 docs\n",
      "Committing\n",
      "Indexed 2060000 docs\n",
      "Indexed 2070000 docs\n",
      "Indexed 2080000 docs\n",
      "Indexed 2090000 docs\n",
      "Indexed 2100000 docs\n",
      "Committing\n",
      "Indexed 2110000 docs\n",
      "Indexed 2120000 docs\n",
      "Indexed 2130000 docs\n",
      "Indexed 2140000 docs\n",
      "Indexed 2150000 docs\n",
      "Committing\n",
      "Indexed 2160000 docs\n",
      "Indexed 2170000 docs\n",
      "Indexed 2180000 docs\n",
      "Indexed 2190000 docs\n",
      "Indexed 2200000 docs\n",
      "Committing\n",
      "Indexed 2210000 docs\n",
      "Indexed 2220000 docs\n",
      "Indexed 2230000 docs\n",
      "Indexed 2240000 docs\n",
      "Indexed 2250000 docs\n",
      "Committing\n",
      "Indexed 2260000 docs\n",
      "Indexed 2270000 docs\n",
      "Indexed 2280000 docs\n",
      "Indexed 2290000 docs\n",
      "Indexed 2300000 docs\n",
      "Committing\n",
      "Indexed 2310000 docs\n",
      "Indexed 2320000 docs\n",
      "Indexed 2330000 docs\n",
      "Indexed 2340000 docs\n",
      "Indexed 2350000 docs\n",
      "Committing\n",
      "Indexed 2360000 docs\n",
      "Indexed 2370000 docs\n",
      "Indexed 2380000 docs\n",
      "Indexed 2390000 docs\n",
      "Indexed 2400000 docs\n",
      "Committing\n",
      "Indexed 2410000 docs\n",
      "Indexed 2420000 docs\n",
      "Indexed 2430000 docs\n",
      "Indexed 2440000 docs\n",
      "Indexed 2450000 docs\n",
      "Committing\n",
      "Indexed 2460000 docs\n",
      "Indexed 2470000 docs\n",
      "Indexed 2480000 docs\n",
      "Indexed 2490000 docs\n",
      "Indexed 2500000 docs\n",
      "Committing\n",
      "Indexed 2510000 docs\n",
      "Indexed 2520000 docs\n",
      "Indexed 2530000 docs\n",
      "Indexed 2540000 docs\n",
      "Indexed 2550000 docs\n",
      "Committing\n",
      "Indexed 2560000 docs\n",
      "Indexed 2570000 docs\n",
      "Indexed 2580000 docs\n",
      "Indexed 2590000 docs\n",
      "Indexed 2600000 docs\n",
      "Committing\n",
      "Indexed 2610000 docs\n",
      "Indexed 2620000 docs\n",
      "Indexed 2630000 docs\n",
      "Indexed 2640000 docs\n",
      "Indexed 2650000 docs\n",
      "Committing\n",
      "Indexed 2660000 docs\n",
      "Indexed 2670000 docs\n",
      "Indexed 2680000 docs\n",
      "Indexed 2690000 docs\n",
      "Indexed 2700000 docs\n",
      "Committing\n",
      "Indexed 2710000 docs\n",
      "Indexed 2720000 docs\n",
      "Indexed 2730000 docs\n",
      "Indexed 2740000 docs\n",
      "Indexed 2750000 docs\n",
      "Committing\n",
      "Indexed 2760000 docs\n",
      "Indexed 2770000 docs\n",
      "Indexed 2780000 docs\n",
      "Indexed 2790000 docs\n",
      "Indexed 2800000 docs\n",
      "Committing\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!scripts/index/create_lucene_index.sh msmarco_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward indices (text is not really necessary for this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts/index/create_fwd_index.sh msmarco_doc mapdb \"text:parsedText text_raw:raw\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-07 13:44:14--  http://boytsov.info/models/msmarco_doc/2019/bert_vanilla/model.best\n",
      "Resolving boytsov.info (boytsov.info)... 69.60.127.165\n",
      "Connecting to boytsov.info (boytsov.info)|69.60.127.165|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438863972 (419M) [text/plain]\n",
      "Saving to: ‘model.best’\n",
      "\n",
      "model.best          100%[===================>] 418.53M  3.08MB/s    in 2m 26s  \n",
      "\n",
      "2021-02-07 13:46:41 (2.86 MB/s) - ‘model.best’ saved [438863972/438863972]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget boytsov.info/models/msmarco_doc/2019/bert_vanilla/model.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we do inference on CPU, which is pretty slow. To use a GPU change the ``DEVICE_NAME``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaBertRanker(\n",
       "  (bert): CustomBertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (cls): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#DEVICE_NAME='cuda:0'\n",
    "MAX_QUERY_LEN=32\n",
    "MAX_DOC_LEN=512 - 32 - 3\n",
    "BATCH_SIZE=16\n",
    "DEVICE_NAME='cpu'\n",
    "MODEL_FILE='model.best'\n",
    "model=torch.load(MODEL_FILE, map_location='cpu')\n",
    "model.to(DEVICE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference/API demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION='msmarco_doc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOCNO': '961921',\n",
       " 'text': 'national park system establish',\n",
       " 'text_raw': 'when was the national park system established',\n",
       " 'text_bert_tok': 'when was the national park system established'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_JSON={\"DOCNO\": \"961921\", \n",
    "            \"text\": \"national park system establish\",\n",
    "             \"text_raw\": \"when was the national park system established\", \"text_bert_tok\": \"when was the national park system established\"}\n",
    "QUERY_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.config import DOCID_FIELD, TEXT_FIELD_NAME, TEXT_RAW_FIELD_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.setup import *\n",
    "# add Java JAR to the class path\n",
    "configure_classpath('target')\n",
    "# create a resource manager\n",
    "resource_manager=create_featextr_resource_manager(f'collections/{COLLECTION}/forward_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.cand_provider import *\n",
    "# create a candidate provider/generator\n",
    "cand_prov = create_cand_provider(resource_manager, PROVIDER_TYPE_LUCENE, f'collections/{COLLECTION}/lucene_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('961921',\n",
       " (1204206,\n",
       "  [CandidateEntry(doc_id='D2527574', score=18.659997940063477),\n",
       "   CandidateEntry(doc_id='D2398015', score=18.492298126220703),\n",
       "   CandidateEntry(doc_id='D1578785', score=18.234092712402344),\n",
       "   CandidateEntry(doc_id='D2189735', score=18.2298583984375),\n",
       "   CandidateEntry(doc_id='D1578782', score=17.947647094726562),\n",
       "   CandidateEntry(doc_id='D2527573', score=17.892498016357422),\n",
       "   CandidateEntry(doc_id='D1578784', score=17.88416862487793),\n",
       "   CandidateEntry(doc_id='D2106902', score=17.869140625),\n",
       "   CandidateEntry(doc_id='D2591882', score=17.70314598083496),\n",
       "   CandidateEntry(doc_id='D2443070', score=17.63814926147461),\n",
       "   CandidateEntry(doc_id='D1578783', score=17.51651382446289),\n",
       "   CandidateEntry(doc_id='D3525662', score=17.447235107421875),\n",
       "   CandidateEntry(doc_id='D2769926', score=17.322866439819336),\n",
       "   CandidateEntry(doc_id='D1737386', score=17.243505477905273),\n",
       "   CandidateEntry(doc_id='D1514002', score=17.16539192199707),\n",
       "   CandidateEntry(doc_id='D14552', score=17.148212432861328),\n",
       "   CandidateEntry(doc_id='D2443068', score=17.124448776245117),\n",
       "   CandidateEntry(doc_id='D2893132', score=17.09151268005371),\n",
       "   CandidateEntry(doc_id='D1581803', score=17.08568572998047),\n",
       "   CandidateEntry(doc_id='D1462277', score=17.0462589263916)]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text=QUERY_JSON[TEXT_FIELD_NAME]\n",
    "query_id=QUERY_JSON[DOCID_FIELD]\n",
    "query_res=run_text_query(cand_prov, 20, query_text)\n",
    "query_id, query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve a document (D1578782 is marked as a relevant entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.py_flexneuart.fwd_index import get_forward_index\n",
    "raw_indx = get_forward_index(resource_manager, 'text_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_ID='D1578782' # relevant\n",
    "#DOC_ID='D1462277' # not marked as relevant\n",
    "doc_text=raw_indx.get_doc_raw(DOC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national park system establish\n",
      "\n",
      "national park mashups \"national park service's 100 year birthday is in 2016. august 25, 2016 is the 100th birthday of the national park service. starting with yellowstone in 1872 there are over 400 units in the national park service today. how old is the system? the national park service was created by an act of congress and signed by president woodrow wilson on august 25, 1916. yellowstone national park was established by an act signed by president ulysses s. grant on march 1, 1872, as the nation's first national park. the mission of the national park service: the national park service preserves unimpaired the natural and cultural resources and values of the national park system for the enjoyment, education, and inspiration of this and future generations. the national park service cooperates with partners to extend the benefits of natural and cultural resource conservation and outdoor recreation throughout this country and the world. national park mashups news, videos, tweets, pictures, map blue ridge parkway cuyahoga valley national park grand canyon national park grand teton national park great smoky mountains national park olympic national park rocky mountain national park white sands national monument yellowstone national park yosemite national park zion national park how many areas are in the national park system? the national park system comprises 401 areas called \"\"units\"\" covering more than 84 million acres. these units include national parks, monuments, battlefields, military parks, historical parks, historic sites, lakeshores, recreation areas, scenic rivers and trails, and the white house. who are the people of the national park service? the national park service employs approximately 20,000 diverse professionals – permanent, temporary, and seasonal. they are assisted by nearly 140,000 volunteers-in-parks (vips) who donate over 5 million hours each year.©2006-present. all rights reserved. this site is not affiliated with the national park service, the us government or any us government agency. \"\n"
     ]
    }
   ],
   "source": [
    "print(query_text)\n",
    "print()\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score candidate documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_data = {}\n",
    "bm25_scores = {}\n",
    "for doc_id, bm25_score in query_res[1]:\n",
    "    doc_text = raw_indx.get_doc_raw(doc_id)\n",
    "    doc_data[doc_id] = doc_text\n",
    "    bm25_scores[doc_id] = bm25_score\n",
    "\n",
    "query_data = {query_id : query_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961921 D2527574 BM25 score: 18.659997940063477 model score: 1.320546269416809\n",
      "961921 D2398015 BM25 score: 18.492298126220703 model score: 0.9334409236907959\n",
      "961921 D1578785 BM25 score: 18.234092712402344 model score: 2.141911029815674\n",
      "961921 D2189735 BM25 score: 18.2298583984375 model score: -0.07196071743965149\n",
      "961921 D1578782 BM25 score: 17.947647094726562 model score: 0.38077396154403687\n",
      "961921 D2527573 BM25 score: 17.892498016357422 model score: 0.9013162851333618\n",
      "961921 D1578784 BM25 score: 17.88416862487793 model score: 1.047125220298767\n",
      "961921 D2106902 BM25 score: 17.869140625 model score: -2.150390386581421\n",
      "961921 D2591882 BM25 score: 17.70314598083496 model score: 1.2829278707504272\n",
      "961921 D2443070 BM25 score: 17.63814926147461 model score: 0.7396841645240784\n",
      "961921 D1578783 BM25 score: 17.51651382446289 model score: 0.8640072345733643\n",
      "961921 D3525662 BM25 score: 17.447235107421875 model score: 1.0577561855316162\n",
      "961921 D2769926 BM25 score: 17.322866439819336 model score: 0.5723984837532043\n",
      "961921 D1737386 BM25 score: 17.243505477905273 model score: 0.17278951406478882\n",
      "961921 D1514002 BM25 score: 17.16539192199707 model score: -2.0407660007476807\n",
      "961921 D14552 BM25 score: 17.148212432861328 model score: -2.3776450157165527\n",
      "961921 D2443068 BM25 score: 17.124448776245117 model score: 1.8065520524978638\n",
      "961921 D2893132 BM25 score: 17.09151268005371 model score: -2.4445197582244873\n",
      "961921 D1581803 BM25 score: 17.08568572998047 model score: -0.9361811876296997\n",
      "961921 D1462277 BM25 score: 17.0462589263916 model score: -1.8627537488937378\n"
     ]
    }
   ],
   "source": [
    "from scripts.cedr.data import iter_valid_records\n",
    "\n",
    "data_set = query_data, doc_data\n",
    "run = {query_id : doc_data.keys()}\n",
    "\n",
    "for records in iter_valid_records(model, DEVICE_NAME, data_set, run,\n",
    "                                       BATCH_SIZE,\n",
    "                                       MAX_QUERY_LEN, MAX_DOC_LEN):\n",
    "    scores = model(records['query_tok'],\n",
    "                    records['query_mask'],\n",
    "                    records['doc_tok'],\n",
    "                    records['doc_mask'])\n",
    "    \n",
    "    \n",
    "    scores = scores.tolist()\n",
    "\n",
    "    for qid, doc_id, score in zip(records['query_id'], records['doc_id'], scores):\n",
    "        print(f'{qid} {doc_id} BM25 score: {bm25_scores[doc_id]} model score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the document against the query (under the hood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2120, 2380, 2291, 5323]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_bert_tok = model.tokenize(query_text)\n",
    "query_bert_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10117, 8573, 1998, 5680, 1000, 1000, 1000, 2057, 2031, 5357, 15891, 2000, 1996, 2087, 14013, 4348, 1037, 2111, 2412, 2363, 1010, 1998, 2169, 2028, 2442, 2079, 2010, 2112, 2065, 2057, 4299, 2000, 2265, 2008, 1996, 3842, 2003, 11007, 1997, 2049, 2204, 7280, 1012, 1000, 1000, 1011, 10117, 8573, 8573, 2018, 2023, 3746, 2579, 2005, 1996, 3104, 1997, 2010, 2338, 1010, 1000, 1000, 5933, 9109, 1997, 1037, 8086, 2386, 1012, 1000, 1000, 17590, 2110, 2118, 10117, 8573, 2003, 2411, 2641, 1996, 1000, 1000, 5680, 2923, 2343, 1012, 1000, 1000, 2182, 1999, 1996, 2167, 7734, 2919, 8653, 1010, 2073, 2116, 1997, 2010, 3167, 5936, 2034, 2435, 4125, 2000, 2010, 2101, 4483, 4073, 1010, 8573, 2003, 4622, 2007, 1037, 2120, 2380, 2008, 6468, 2010, 2171, 1998, 7836, 1996, 3638, 1997, 2023, 2307, 5680, 2923, 1012, 10117, 8573, 2034, 2234, 2000, 1996, 2919, 8653, 1999, 2244, 7257, 1012, 1037, 27168, 1011, 4477, 2035, 2010, 2166, 1010, 8573, 4912, 1037, 3382, 2000, 5690, 1996, 2502, 2208, 1997, 2167, 2637, 2077, 2027, 5419, 1012, 2348, 2010, 7896, 17120, 3365, 5933, 9109, 1998, 3144, 8563, 1010, 2027, 2024, 17958, 2007, 20342, 3372, 2005, 1996, 3279, 1997, 2427, 1998, 6552, 1012, 1996, 11703, 9581, 3508, 1997, 22285, 1010, 1998, 1996, 3690, 25027, 1997, 18995, 1010, 2502, 9769, 8351, 1010, 8448, 1998, 2060, 2208, 2427, 2001, 1037, 3279, 2029, 8573, 2371, 24668, 1997, 2554, 1005, 1055, 10617, 1997, 2256, 3019, 4219, 1012, 2002, 2387, 1996, 3896, 1997, 2058, 17643, 6774, 1010, 1998, 4265, 1996, 3279, 1997, 2010, 8086, 2229, 2138, 1997, 2009, 1012, 2096, 2116, 2145, 2641, 3019, 4219, 1999, 10288, 13821, 3775, 3468, 1010, 8573, 2052, 4339, 1024, 2057, 2031, 2468, 2307, 2138, 1997, 1996, 22689, 2224, 1997, 2256, 4219, 1012, 2021, 1996, 2051, 2038, 2272, 2000, 1999, 15549, 2890, 5667, 2054, 2097, 4148, 2043, 2256, 6138, 2024, 2908, 1010, 2043, 1996, 5317, 1010, 1996, 3707, 1010, 1996, 3514, 1010, 1998, 1996, 3806, 2024, 9069, 1010, 2043, 1996, 13622, 2031, 2145, 2582, 25488, 1998, 8871, 2046, 1996, 9199, 1010, 8554, 20807, 1996, 5485, 1010, 7939, 24539, 1996, 4249, 1998, 27885, 3367, 6820, 11873, 9163, 1012, 5680, 6233, 2150, 2028, 1997, 8573, 1005, 1055, 2364, 5936, 1012, 2044, 3352, 2343, 1999, 5775, 1010, 8573, 2109, 2010, 3691, 2000, 4047, 6870, 1998, 2270, 4915, 2011, 4526, 1996, 2142, 2163, 3224, 2326, 1006, 2149, 10343, 1007, 1998, 7411, 5018, 2120, 6138, 1010, 4868, 2976, 4743, 8269, 1010, 1018, 2120, 2208, 18536, 1010, 1019, 2120, 6328, 1010, 1998, 2324, 2120, 10490, 2011, 12067, 1996, 5518, 2137, 21387, 2552, 1012, 2076, 2010, 8798, 1010, 10117, 8573, 5123, 3155, 11816, 2454, 4631, 1997, 2270, 2455, 1012, 2651, 1010, 1996, 8027, 1997, 10117, 8573, 2003, 2179, 2408, 1996, 2406, 1012, 2045, 2024, 2416, 2120, 2380, 4573, 4056, 1010, 1999, 2112, 2030, 2878, 1010, 2000, 2256, 5680, 2923, 2343, 1012, 2017, 2064, 2424, 2062, 2592, 2055, 2122, 3182, 2104, 10117, 8573, 3141, 11744, 1012, 2270, 4915, 2511, 2011, 10117, 8573, 1996, 5680, 8027, 1997, 10117, 8573, 2003, 2179, 1999, 1996, 11816, 2454, 4631, 1997, 2270, 4915, 2002, 3271, 5323, 2076, 2010, 8798, 1012, 2172, 1997, 2008, 2455, 1011, 5018, 8817, 4631, 1011, 2001, 2275, 4998, 2004, 2120, 6138, 1012, 8573, 2580, 1996, 2556, 1011, 2154, 2149, 10343, 1999, 5497, 1010, 2019, 3029, 2306, 1996, 2533, 1997, 5237, 1012, 1996, 2801, 2001, 2000, 27749, 6138, 2005, 2506, 2224, 1012, 2019, 29502, 22488, 1997, 16911, 1996, 2406, 1005, 1055, 4219, 1010, 8573, 2359, 2000, 16021, 5397, 1996, 15169, 1997, 2216, 4219, 1012, 8573, 2001, 2036, 1996, 2034, 2343, 2000, 3443, 1037, 2976, 4743, 3914, 1010, 1998, 2002, 2052, 5323, 4868, 1997, 2122, 2076, 2010, 3447, 1012, 2122, 8269, 2052, 2101, 2468, 2651, 1005, 1055, 2120, 6870, 9277, 2015, 1010, 3266, 2011, 1996, 2142, 2163, 3869, 1998, 6870, 2326, 1006, 2149, 2546, 9333, 1007, 1012, 2651, 2045, 2003, 1037, 2120, 6870, 9277, 1999, 2296, 2110, 1010, 1998, 2167, 7734, 21979, 1996, 2087, 9277, 2015, 1997, 2151, 2110, 1999, 1996, 2406, 1012, 2076, 8573, 1005, 1055, 3447, 1010, 1996, 2120, 2380, 2291, 3473, 12381, 1012, 2043, 1996, 2120, 2380, 2326, 2001, 2580, 1999, 4947, 1011, 2698, 2086, 2044, 8573, 2187, 2436, 1011, 2045, 2020, 3486, 4573, 2000, 2022, 3266, 2011, 1996, 2047, 3029, 1012, 8573, 3271, 2580, 2603, 1997, 2216, 1012, 2156, 2917, 2005, 1037, 2862, 1997, 1996, 4573, 2580, 2076, 2010, 3447, 2029, 2024, 4198, 2007, 1996, 2120, 2380, 2326, 1012, 2120, 6328, 2120, 6328, 2024, 2580, 2011, 2019, 2552, 1997, 3519, 1012, 2077, 4947, 1010, 2027, 2020, 3266, 2011, 1996, 3187, 1997, 1996, 4592, 1012, 8573, 2499, 2007, 2010, 4884, 3589, 2000, 5323, 2122, 4573, 1024, 11351, 2697, 2120, 2380, 1006, 2030, 1007, 1011, 5774, 11101, 5430, 2120, 2380, 1006, 17371, 1007, 1011, 5778, 23722, 2135, 2015, 2940, 1006, 1050, 2094, 1007, 1011, 5692, 1006, 2085, 3266, 2011, 2149, 2546, 9333, 1007, 28005, 2120, 2380, 1006, 7929, 1007, 1011, 5518, 1006, 2085, 2112, 1997, 14556, 16782, 2860, 2120, 8640, 2181, 1007, 15797, 16184, 2120, 2380, 1006, 2522, 1007, 1011, 5518, 4215, 5732, 2455, 2000, 10930, 3366, 23419, 2120, 2380, 1006, 6187, 1007, 2120, 10490, 8573, 2772, 1996, 2552, 2005, 1996, 8347, 1997, 2137, 21387, 1011, 2036, 2124, 2004, 1996, 21387, 2552, 2030, 1996, 2120, 10490, 2552, 1011, 2006, 2238, 1022, 1010, 5518, 1012, 1996, 2375, 2435, 1996, 2343, 19258, 2000, 1000, 1000, 13520, 2011, 2270, 16413, 3181, 16209, 1010, 3181, 1998, 14491, 5090, 1010, 1998, 2060, 5200, 1997, 3181, 1998, 4045, 3037, 1012, 1012, 1012, 2000, 2022, 2120, 10490, 1012, 1000, 1000, 2144, 2002, 2106, 2025, 2342, 7740, 6226, 1010, 8573, 2071, 5323, 2120, 10490, 2172, 6082, 2084, 2120, 6328, 1012, 2002, 4056, 2122, 4573, 2004, 2120, 10490, 1024, 6548, 1005, 1055, 3578, 1006, 1059, 2100, 1007, 1011, 5518, 2884, 22822, 3217, 1006, 13221, 1007, 1011, 5518, 9629, 9351, 12248, 3317, 1006, 17207, 1007, 1011, 5518, 22327, 22618, 3224, 1006, 17207, 1007, 1011, 5518, 1006, 2085, 1037, 2120, 2380, 1007, 15775, 3597, 8399, 1006, 13221, 1007, 1011, 5528, 27102, 2368, 4672, 1006, 6187, 1007, 1011, 5528, 1006, 2085, 27333, 2368, 10942, 2120, 2380, 1007, 29399, 13171, 1006, 6187, 1007, 1011, 5528, 1006, 2085, 2112, 1997, 27333, 2368, 10942, 2120, 2380, 1007, 13097, 2050, 7656, 16707, 1006, 13221, 1007, 1011, 5528, 2669, 3406, 1006, 17207, 1007, 1011, 5528, 12274, 4313, 5249, 1006, 6187, 1007, 1011, 5316, 17643, 4859, 8399, 1006, 17207, 1007, 1011, 5316, 1006, 2085, 1037, 2120, 2380, 1007, 26007, 2015, 1006, 6187, 1007, 1011, 5316, 1006, 2085, 1037, 2120, 2380, 1007, 13713, 5430, 1006, 17371, 1007, 1011, 5316, 19833, 11137, 7346, 1006, 21183, 1007, 1011, 5316, 2571, 9148, 2015, 1004, 5215, 16679, 2015, 1006, 11047, 1007, 1011, 5316, 1006, 2085, 1037, 8124, 2110, 2380, 1007, 10722, 22911, 22684, 3089, 1006, 17207, 1007, 1011, 5316, 22920, 2121, 1006, 2522, 1007, 1011, 5316, 1006, 2085, 12819, 22125, 2181, 1010, 2112, 1997, 5673, 9026, 2120, 3224, 1007, 4057, 26742, 1006, 11333, 1007, 1011, 5556, 1006, 2085, 4386, 2120, 2380, 1007, 8573, 2036, 2511, 15775, 13728, 7585, 6104, 1998, 5286, 1999, 5528, 1010, 1037, 2609, 1997, 1996, 2645, 1997, 2047, 5979, 1012, 2009, 2003, 2085, 1037, 2112, 1997, 3744, 2474, 8873, 4674, 2120, 3439, 2380, 1012, 8573, 7896, 2006, 5680, 10117, 8573, 2001, 1996, 2034, 2343, 1997, 1996, 16430, 1010, 1037, 2051, 1997, 2307, 4935, 1998, 2458, 1012, 2010, 13347, 2000, 9530, 8043, 6455, 2256, 3019, 1998, 3451, 2381, 3271, 5323, 1037, 20056, 2012, 2019, 2590, 2051, 1999, 2256, 3842, 1005, 1055, 2381, 1012, 2043, 2116, 2145, 2641, 2256, 4219, 1999, 10288, 13821, 3775, 3468, 1010, 8573, 2387, 2068, 2004, 2242, 2000, 4047, 1998, 24188, 4509, 1024, 2009, 2003, 2036, 3158, 9305, 2964, 2215, 2239, 2135, 2000, 6033, 2030, 2000, 9146, 1996, 6215, 1997, 2054, 2003, 3376, 1999, 3267, 1010, 3251, 2009, 2022, 1037, 7656, 1010, 1037, 3224, 1010, 2030, 1037, 2427, 1997, 25476, 2030, 4743, 1012, 2182, 1999, 1996, 2142, 2163, 2057, 2735, 2256, 5485, 1998, 9199, 2046, 22365, 2015, 1998, 23642, 1011, 5286, 1010, 2057, 8554, 10421, 1996, 2250, 1010, 2057, 6033, 6138, 1010, 1998, 4654, 3334, 19269, 21995, 1010, 5055, 1998, 11993, 1011, 1011, 2025, 2000, 3713, 1997, 29364, 6026, 11951, 12793, 2007, 22293, 14389, 1012, 2021, 2012, 2197, 2009, 3504, 2004, 2065, 2256, 2111, 2020, 16936, 1012, 1996, 2307, 8347, 2923, 2198, 23110, 1010, 4986, 2058, 1996, 6215, 1997, 2530, 2752, 1010, 4778, 2343, 8573, 2000, 3409, 1999, 10930, 3366, 23419, 2120, 2380, 1012, 2044, 2010, 4440, 1010, 8573, 10783, 1024, 1000, 1000, 2009, 2001, 2066, 4688, 1999, 1037, 2307, 19487, 5040, 1010, 2521, 6565, 2121, 1998, 2062, 3376, 2084, 2151, 2328, 2011, 1996, 2192, 1997, 2158, 1012, 1000, 1000, 2002, 3024, 1037, 4675, 1011, 5703, 2000, 2216, 2040, 4912, 2000, 18077, 1996, 3019, 2088, 2005, 3167, 5114, 1012, 2043, 3519, 4061, 2010, 4073, 2000, 3443, 1037, 2120, 2380, 2012, 1996, 2882, 8399, 1010, 8573, 2109, 2010, 3237, 2373, 2000, 4047, 2009, 2004, 1037, 2120, 6104, 1024, 1999, 1996, 2882, 8399, 1010, 5334, 2038, 1037, 3019, 4687, 2029, 2003, 1999, 2785, 7078, 4895, 28689, 6216, 3709, 2802, 1996, 2717, 1997, 1996, 2088, 1012, 1045, 2215, 2000, 3198, 2017, 2000, 2562, 2023, 2307, 4687, 1997, 3267, 2004, 2009, 2085, 2003, 1012, 1045, 3246, 2017, 2097, 2025, 2031, 1037, 2311, 1997, 2151, 2785, 1010, 2025, 1037, 2621, 9151, 1010, 1037, 3309, 2030, 2505, 2842, 1010, 2000, 9388, 1996, 6919, 9026, 3126, 1010, 1996, 4942, 17960, 3012, 1010, 1996, 2307, 20334, 1998, 5053, 1997, 1996, 8399, 1012, 2681, 2009, 2004, 2009, 2003, 1012, 2017, 3685, 5335, 2006, 2009, 1012, 1996, 5535, 2031, 2042, 2012, 2147, 2006, 2009, 1010, 1998, 2158, 2064, 2069, 9388, 2009, 1012, 2017, 2064, 3942, 2256, 4037, 2000, 3191, 2062, 1997, 8573, 1005, 1055, 16614, 2006, 3267, 1998, 5680, 1012, 1000] 1594\n"
     ]
    }
   ],
   "source": [
    "doc_bert_tok = model.tokenize(doc_text)\n",
    "print(doc_bert_tok, len(doc_bert_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to truncate queries and documents ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bert_tok=query_bert_tok[0:MAX_QUERY_LEN]\n",
    "doc_bert_tok=doc_bert_tok[0:MAX_DOC_LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and pad queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2120, 2380, 2291, 5323, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "from scripts.cedr.data import PAD_CODE\n",
    "\n",
    "query_bert_tok_pad = query_bert_tok + [PAD_CODE] * (MAX_QUERY_LEN - len(query_bert_tok))\n",
    "print(query_bert_tok_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call unsqueeze(0) is required to create a batch dimension (we can have multiple queries & documents batched together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 477)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tok_tensor_pad = torch.LongTensor(query_bert_tok_pad).unsqueeze(0).to(DEVICE_NAME)\n",
    "doc_tok_tensor = torch.LongTensor(doc_bert_tok).unsqueeze(0).to(DEVICE_NAME)\n",
    "len(query_tok_tensor_pad[0]), len(doc_tok_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32]), torch.Size([1, 477]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tok_tensor_pad.shape, doc_tok_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mask = torch.FloatTensor([1.0] * len(query_bert_tok) + \n",
    "                              [0.] * (MAX_QUERY_LEN - len(query_bert_tok))).unsqueeze(0).to(DEVICE_NAME)\n",
    "doc_mask = torch.ones_like(doc_tok_tensor).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32]), torch.Size([1, 477]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_mask.shape, doc_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_mask, doc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8628], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(query_tok_tensor_pad, query_mask, doc_tok_tensor, doc_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}